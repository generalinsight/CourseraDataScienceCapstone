#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[1]])
as.character(mycorpus[[2]])
con <- file('en_US.twitter.txt', 'r')
mydata <- readLines(con,500)
mycorpus <- Corpus(VectorSource(mydata))
#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[2]])
mycorpus
readline(mycorpus)
library(ggplot2)
library(wordcloud)
library(RWeka)
con <- file('en_US.twitter.txt', 'r')
mydata <- readLines(con,500)
mycorpus <- Corpus(VectorSource(mydata))
#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[1]])
#bigrams
minfreq_bigram <-2
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(mycorpus, Weka_control(min=2, max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <-two_word[order(two_word$Freq, decreasing = True),]
wordcloud(sort_two$bitoken, sort_two$Freq, random, order = FALSE, scale = c(2, 0.35), min.freq = minfreq_bigram, colors = brewer.pal)
con <- file('en_US.twitter.txt', 'r')
mydata <- readLines(con,500)
mycorpus <- Corpus(VectorSource(mydata))
#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[1]])
#bigrams
minfreq_bigram <-2
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(mycorpus, Weka_control(min=2, max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq, decreasing = TRUE),]
wordcloud(sort_two$bitoken, sort_two$Freq, random, order = FALSE, scale = c(2, 0.35), min.freq = minfreq_bigram, colors = brewer.pal)
con <- file('en_US.twitter.txt', 'r')
mydata <- readLines(con,500)
mycorpus <- Corpus(VectorSource(mydata))
#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[1]])
#bigrams
minfreq_bigram <-2
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(mycorpus, Weka_control(min=2, max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq, decreasing = TRUE),]
wordcloud(sort_two$bitoken, sort_two$Freq, random.order = FALSE, scale = c(2, 0.35), min.freq = minfreq_bigram, colors = brewer.pal)
View(sort_two)
con <- file('en_US.twitter.txt', 'r')
mydata <- readLines(con,500)
mycorpus <- Corpus(VectorSource(mydata))
#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[1]])
#bigrams
minfreq_bigram <-2
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(mycorpus, Weka_control(min=2, max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq, decreasing = TRUE),]
wordcloud(sort_two$bitoken, sort_two$Freq, random.order = FALSE, scale = c(2, 0.35), min.freq = minfreq_bigram, colors = brewer.pal(8, "Dark2"), max.words = 150)
#trigrams
minfreq_trigram <- 5
token_delim <- " \\t\\r\\n.!?,;\"()"
tritoken <- NGramTokenizer(mycorpus, Weka_control(min=3, max=3, delimiters = token_delim))
three_word <- data.frame(table(tritoken))
sort_three <- three_word[order(three_word$Freq, decreasing = TRUE),]
wordcloud(sort_three$tritoken, sort_three$Freq, random.order = FALSE, scale = c(2, 0.35), min.freq = minfreq_trigram, colors = brewer.pal(8, "Dark2"), max.words = 150)
con <- file('en_US.twitter.txt', 'r')
mydata <- readLines(con,100)
mycorpus <- Corpus(VectorSource(mydata))
#text cleaning
#convert to lower case
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
#remove number
mycorpus <- tm_map(mycorpus, removeNumbers)
#remove English common stopwords
mycorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
#remove punctuations
mycorpus <- tm_map(mycorpus, removePunctuation)
#eliminate extra white space
mycorpus <- tm_map(mycorpus, stripWhitespace)
as.character(mycorpus[[1]])
#bigrams
minfreq_bigram <- 2
token_delim <- " \\t\\r\\n.!?,;\"()"
bitoken <- NGramTokenizer(mycorpus, Weka_control(min=2, max=2, delimiters = token_delim))
two_word <- data.frame(table(bitoken))
sort_two <- two_word[order(two_word$Freq, decreasing = TRUE),]
wordcloud(sort_two$bitoken, sort_two$Freq, random.order = FALSE, scale = c(2, 0.35), min.freq = minfreq_bigram, colors = brewer.pal(8, "Dark2"), max.words = 150)
#trigrams
minfreq_trigram <- 5
token_delim <- " \\t\\r\\n.!?,;\"()"
tritoken <- NGramTokenizer(mycorpus, Weka_control(min=3, max=3, delimiters = token_delim))
three_word <- data.frame(table(tritoken))
sort_three <- three_word[order(three_word$Freq, decreasing = TRUE),]
wordcloud(sort_three$tritoken, sort_three$Freq, random.order = FALSE, scale = c(2, 0.35), min.freq = minfreq_trigram, colors = brewer.pal(8, "Dark2"), max.words = 150)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
# N-gram upto 10-gram
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
gram2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
gram3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
gram4 <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
gram5 <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
gram6 <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))
gram7 <- function(x) NGramTokenizer(x, Weka_control(min = 7, max = 7))
gram8 <- function(x) NGramTokenizer(x, Weka_control(min = 8, max = 8))
gram9 <- function(x) NGramTokenizer(x, Weka_control(min = 9, max = 9))
gram10 <- function(x) NGramTokenizer(x, Weka_control(min = 10, max = 10))
# aggregate frequencies from mycorpus
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data")
f2 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram2, bounds = list(global = c(5, Inf)))))
save(f2, file="f2data")
f3 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram3, bounds = list(global = c(3, Inf)))))
save(f3, file="f3data")
f4 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram4, bounds = list(global = c(2, Inf)))))
save(f4, file="f4data")
f5 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram5, bounds = list(global = c(2, Inf)))))
save(f5, file="f5data")
f6 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram6, bounds = list(global = c(2, Inf)))))
save(f6, file="f6data")
f7 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram7, bounds = list(global = c(2, Inf)))))
save(f7, file="f7data")
f8 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram8, bounds = list(global = c(2, Inf)))))
save(f8, file="f8data")
f9 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram9, bounds = list(global = c(2, Inf)))))
save(f9, file="f9data")
f10 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram10, bounds = list(global = c(2, Inf)))))
save(f10, file="f10data")
allf <- list("f1" = f1, "f2" = f2, "f3" = f3, "f4" = f4, "f5" = f5, "f6" = f6, "f7" = f7, "f8" = f8, "f9" = f9, "f10" = f10)
save(allf, file="allfdata")
# N-gram upto 10-gram
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
gram2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
gram3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
gram4 <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
gram5 <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
gram6 <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))
gram7 <- function(x) NGramTokenizer(x, Weka_control(min = 7, max = 7))
gram8 <- function(x) NGramTokenizer(x, Weka_control(min = 8, max = 8))
gram9 <- function(x) NGramTokenizer(x, Weka_control(min = 9, max = 9))
gram10 <- function(x) NGramTokenizer(x, Weka_control(min = 10, max = 10))
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data")
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data")
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data")
f2 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram2, bounds = list(global = c(5, Inf)))))
save(f2, file="f2data")
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
library(slam)
# N-gram upto 10-gram
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
gram2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
gram3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
gram4 <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
gram5 <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
gram6 <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))
gram7 <- function(x) NGramTokenizer(x, Weka_control(min = 7, max = 7))
gram8 <- function(x) NGramTokenizer(x, Weka_control(min = 8, max = 8))
gram9 <- function(x) NGramTokenizer(x, Weka_control(min = 9, max = 9))
gram10 <- function(x) NGramTokenizer(x, Weka_control(min = 10, max = 10))
# aggregate frequencies from mycorpus
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data")
con <- file('en_US.blogs.txt', 'r')
blogsdata <- readLines(con, skipNul = TRUE)
close(con)
con <- file('en_US.news.txt', 'r')
newsdata <- readLines(con, skipNul = TRUE)
close(con)
con <- file('en_US.twitter.txt', 'r')
twitterdata <- readLines(con, skipNul = TRUE)
close(con)
# Random sampling of datasets
set.seed(999)
samplerate <- 0.05
sampledata <- c(sample(blogsdata, length(blogsdata) * samplerate),
sample(newsdata, length(newsdata) * samplerate),
sample(twitterdata, length(twitterdata) * samplerate))
# Step through the cleaning processes
myCorpus <- VCorpus(VectorSource(sampledata))
identifier <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, identifier, "(f|ht)tp(s?)://(.*)[.][a-z]+")
myCorpus <- tm_map(myCorpus, identifier, "@[^\\s]+")
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remobve whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower
myCorpus <- tm_map(myCorpus, tolower)
# convert to plain text
myCorpus <- tm_map(myCorpus, PlainTextDocument)
# remove english stopwords
singlecorpus <- tm_map(mycorpus, removeWords, stopwords('english'))
singlecorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
# N-gram upto 10-gram
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
gram2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
gram3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
gram4 <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
gram5 <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
gram6 <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))
gram7 <- function(x) NGramTokenizer(x, Weka_control(min = 7, max = 7))
gram8 <- function(x) NGramTokenizer(x, Weka_control(min = 8, max = 8))
gram9 <- function(x) NGramTokenizer(x, Weka_control(min = 9, max = 9))
gram10 <- function(x) NGramTokenizer(x, Weka_control(min = 10, max = 10))
# aggregate frequencies from mycorpus
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data")
f2 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram2, bounds = list(global = c(5, Inf)))))
save(f2, file="f2data")
f3 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram3, bounds = list(global = c(3, Inf)))))
save(f3, file="f3data")
f4 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram4, bounds = list(global = c(2, Inf)))))
save(f4, file="f4data")
f5 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram5, bounds = list(global = c(2, Inf)))))
save(f5, file="f5data")
f6 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram6, bounds = list(global = c(2, Inf)))))
save(f6, file="f6data")
f7 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram7, bounds = list(global = c(2, Inf)))))
save(f7, file="f7data")
f8 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram8, bounds = list(global = c(2, Inf)))))
save(f8, file="f8data")
f9 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram9, bounds = list(global = c(2, Inf)))))
save(f9, file="f9data")
f10 <- getFreq(TermDocumentMatrix(mycorpus, control = list(tokenize = gram10, bounds = list(global = c(2, Inf)))))
save(f10, file="f10data")
allf <- list("f1" = f1, "f2" = f2, "f3" = f3, "f4" = f4, "f5" = f5, "f6" = f6, "f7" = f7, "f8" = f8, "f9" = f9, "f10" = f10)
save(allf, file="allfdata")
source('~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/ngrams.R', echo=TRUE)
f4 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram4, bounds = list(global = c(2, Inf)))))
save(f4, file="f4data")
f5 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram5, bounds = list(global = c(2, Inf)))))
save(f5, file="f5data")
f6 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram6, bounds = list(global = c(2, Inf)))))
save(f6, file="f6data")
f7 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram7, bounds = list(global = c(2, Inf)))))
save(f7, file="f7data")
f8 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram8, bounds = list(global = c(2, Inf)))))
save(f8, file="f8data")
f9 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram9, bounds = list(global = c(2, Inf)))))
save(f9, file="f9data")
f10 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram10, bounds = list(global = c(2, Inf)))))
save(f10, file="f10data")
allf <- list("f1" = f1, "f2" = f2, "f3" = f3, "f4" = f4, "f5" = f5, "f6" = f6, "f7" = f7, "f8" = f8, "f9" = f9, "f10" = f10)
save(allf, file="allfdata")
allf <- list("f1" = f1, "f2" = f2, "f3" = f3, "f4" = f4, "f5" = f5, "f6" = f6, "f7" = f7, "f8" = f8, "f9" = f9, "f10" = f10)
save(allf, file="allfdata.RData")
load("~/Courses/Data Science Specialization Coursera/CapstoneProject-wk10/DataScienceCourseraCapstone/allfdata.RData")
# Libraries loaded
library(dplyr)
library(tm)
library(ggplot2)
library(RWeka)
library(stringi)
library(knitr)
library(slam)
# Data file and connections established
con <- file('en_US.blogs.txt', 'r')
blogsdata <- readLines(con, skipNul = TRUE)
close(con)
con <- file('en_US.news.txt', 'r')
newsdata <- readLines(con, skipNul = TRUE)
close(con)
con <- file('en_US.twitter.txt', 'r')
twitterdata <- readLines(con, skipNul = TRUE)
close(con)
# Random sampling of datasets
set.seed(999)
samplerate <- 0.03
sampledata <- c(sample(blogsdata, length(blogsdata) * samplerate),
sample(newsdata, length(newsdata) * samplerate),
sample(twitterdata, length(twitterdata) * samplerate))
# Step through the cleaning processes
myCorpus <- VCorpus(VectorSource(sampledata))
identifier <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, identifier, "(f|ht)tp(s?)://(.*)[.][a-z]+")
myCorpus <- tm_map(myCorpus, identifier, "@[^\\s]+")
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remobve whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower
myCorpus <- tm_map(myCorpus, tolower)
# convert to plain text
myCorpus <- tm_map(myCorpus, PlainTextDocument)
# remove english stopwords
singlecorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
# N-gram upto 5-gram
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
gram2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
gram3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
gram4 <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
gram5 <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
# aggregate frequencies from mycorpus
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data.RData")
f2 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram2, bounds = list(global = c(5, Inf)))))
save(f2, file="f2data.RData")
f3 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram3, bounds = list(global = c(3, Inf)))))
save(f3, file="f3data.RData")
f4 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram4, bounds = list(global = c(2, Inf)))))
save(f4, file="f4data.RData")
f5 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram5, bounds = list(global = c(2, Inf)))))
save(f5, file="f5data.RData")
allf <- list("f1" = f1, "f2" = f2, "f3" = f3, "f4" = f4, "f5" = f5)
save(allf, file="allfdata.RData")
load("C:/Users/sombando/Downloads/trigram.RData")
shiny::runApp('wordpredict')
runApp('wordpredict')
df2_KN <- readRDS("C:/Users/sombando/Downloads/df2_KN.rds")
start_word_prediction <- readRDS("C:/Users/sombando/Downloads/start_word_prediction.rds")
head(start_word_prediction, 10)
head(start_word_prediction, 100)
View(gram2)
biGram <- readRDS("C:/Users/sombando/Downloads/biGram.rds")
head(biGram, 20)
head(f2, 10)
# Libraries loaded
library(dplyr)
library(tm)
library(ggplot2)
library(RWeka)
library(stringi)
library(knitr)
library(slam)
# Data file and connections established
con <- file('en_US.blogs.txt', 'r')
blogsdata <- readLines(con, skipNul = TRUE)
close(con)
con <- file('en_US.news.txt', 'r')
newsdata <- readLines(con, skipNul = TRUE)
close(con)
con <- file('en_US.twitter.txt', 'r')
twitterdata <- readLines(con, skipNul = TRUE)
close(con)
# Random sampling of datasets
set.seed(999)
samplerate <- 0.03
sampledata <- c(sample(blogsdata, length(blogsdata) * samplerate),
sample(newsdata, length(newsdata) * samplerate),
sample(twitterdata, length(twitterdata) * samplerate))
# Step through the cleaning processes
myCorpus <- VCorpus(VectorSource(sampledata))
identifier <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, identifier, "(f|ht)tp(s?)://(.*)[.][a-z]+")
myCorpus <- tm_map(myCorpus, identifier, "@[^\\s]+")
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remobve whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower
myCorpus <- tm_map(myCorpus, tolower)
# convert to plain text
myCorpus <- tm_map(myCorpus, PlainTextDocument)
# remove english stopwords
singlecorpus <- tm_map(myCorpus, removeWords, stopwords('english'))
# N-gram upto 5-gram
getFreq <- function(tdm) {
freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
return(data.frame(word = names(freq), freq = freq))
}
gram2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
gram3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
gram4 <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
gram5 <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
# aggregate frequencies from myCorpus
f1 <- getFreq(removeSparseTerms(TermDocumentMatrix(singlecorpus), 0.999))
save(f1, file="f1data.rds")
f2 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram2, bounds = list(global = c(5, Inf)))))
save(f2, file="f2data.rds")
f3 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram3, bounds = list(global = c(3, Inf)))))
save(f3, file="f3data.rds")
f4 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram4, bounds = list(global = c(2, Inf)))))
save(f4, file="f4data.rds")
f5 <- getFreq(TermDocumentMatrix(myCorpus, control = list(tokenize = gram5, bounds = list(global = c(2, Inf)))))
save(f5, file="f5data.rds")
allf <- list("f1" = f1, "f2" = f2, "f3" = f3, "f4" = f4, "f5" = f5)
save(allf, file="allfdata.rds")
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
shiny::runApp('wordpredict')
install.packages("stylo")
runApp('wordpredict')
shiny::runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
runApp('wordpredict')
